{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02a5284e-00d8-4794-a4d3-49352fcd2c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import numpy as np\n",
    "import numpy.ma as ma\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57a45cf1-62aa-490c-8130-6d47ef9394f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages')\n",
    "\n",
    "import spacy\n",
    "import nltk\n",
    "import stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe212d9d-e116-44c2-a0e0-2d2b920455f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_seq(seq, model_type, model=None):\n",
    "    \n",
    "    if model_type == 'spacy':\n",
    "        if not model:\n",
    "            model = spacy.load(\"en_core_web_sm\")\n",
    "        return [(token.text, token.pos_, token.lemma_) for token in model(seq)]\n",
    "\n",
    "class MorphTagger():\n",
    "    \n",
    "    \"\"\"\n",
    "    a class for morhological tagging of English text\n",
    "    args:\n",
    "        model -- callable which tags a sequence (spacy/nltk/stanza)\n",
    "        model_type -- string in ['spacy', 'nltk', 'stanza']\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 model_type,\n",
    "                 model=None):\n",
    "        \n",
    "        self.model = model\n",
    "        self.model_type = model_type\n",
    "        \n",
    "    def load_model(self):\n",
    "        \n",
    "        if not self.model:\n",
    "            if self.model_type == 'spacy':\n",
    "                self.model = spacy.load(\"en_core_web_sm\")\n",
    "            elif self.model_type == 'stanza':\n",
    "                self.model = stanza.Pipeline(lang='en', processors='tokenize,mwt,pos')\n",
    "        \n",
    "    def tag_seq(self, seq, text_only=True):\n",
    "        \n",
    "        if text_only:\n",
    "            return [token.text for token in self.model(seq)]\n",
    "        else:\n",
    "            return [token for token in self.model(seq)]\n",
    "        \n",
    "    def predict(self, seq):\n",
    "        \n",
    "        return [token[1] for token in tag_seq(seq, self.model_type, self.model)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a4f83270-2122-4512-8e96-1ff34ccb394f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "class LinearRules():\n",
    "    \n",
    "    def __init__(self, \n",
    "                 tagger, \n",
    "                 neg_markers=['not', 'no'],\n",
    "                 neg_position=2,\n",
    "                 pst_position=3):\n",
    "        \n",
    "        self.tagger = tagger\n",
    "        self.tagger.load_model()\n",
    "        self.neg_markers = neg_markers\n",
    "        self.neg_position = neg_position\n",
    "        self.pst_position = pst_position\n",
    "        \n",
    "    def shift_negation(self, sent, as_list=False):\n",
    "        \n",
    "        tg_sent = tagger.tag_seq(sent, text_only=False)\n",
    "        is_neg = lambda token: str(token.morph) == 'Polarity=Neg'\n",
    "        neg_mask = np.array([is_neg(token) for token in tg_sent])\n",
    "        if neg_mask.sum() == 0:\n",
    "            return None\n",
    "        lemmatize = lambda token: token.text if token.lemma_ not in self.neg_markers else None\n",
    "        \n",
    "        lm_sent = [lemmatize(token) for token in tg_sent if lemmatize(token)]\n",
    "        position = min(self.pst_position, len(lm_sent)-1)\n",
    "        if position != len(lm_sent)-1:\n",
    "            while (tg_sent[position].pos_ == 'PUNCT') or (tg_sent[position].text == '\\n') and (position < len(lm_sent)-1):\n",
    "                position += 1\n",
    "            lm_sent = lm_sent[:position] + ['not'] + lm_sent[position:]\n",
    "        \n",
    "        if as_list:\n",
    "            return lm_sent\n",
    "        else:\n",
    "            return ' '.join(lm_sent)    \n",
    "        \n",
    "    def question_reverse(self, sent):\n",
    "        if len(sent) >= 1:\n",
    "            if sent[-1] == '?' and re.match(r'\\w', sent):\n",
    "                tr_sent = tagger.tag_seq(sent, text_only=True)[-2::-1] + ['?']\n",
    "                return ' '.join(tr_sent)\n",
    "            else:\n",
    "                return None\n",
    "        \n",
    "    \n",
    "    def shift_past(self, sent, as_list=False):\n",
    "        \n",
    "        tg_sent = tagger.tag_seq(sent, text_only=False)\n",
    "        is_pst_verb = lambda token: str(token.morph) == 'Tense=Past|VerbForm=Fin'\n",
    "        pst_mask = np.array([is_pst_verb(token) for token in tg_sent])\n",
    "        if pst_mask.sum() == 0:\n",
    "            return None\n",
    "        \n",
    "        position = min(self.pst_position, len(tg_sent)-1)\n",
    "        pst_positions = np.arange(len(tg_sent))[pst_mask]\n",
    "        while ((tg_sent[position].pos_ == 'PUNCT') or (tg_sent[position].text == '\\n')) \\\n",
    "            and (position < len(tg_sent)-1):\n",
    "            position += 1\n",
    "                    \n",
    "        def lemmatize(token):\n",
    "            if str(token.morph) == 'Tense=Past|VerbForm=Fin':\n",
    "                if token.i == position:\n",
    "                    return token.text\n",
    "                else:\n",
    "                    return token.lemma_\n",
    "            else:\n",
    "                if token.i == position:\n",
    "                    return token.lemma_ + 'ed'\n",
    "                else:\n",
    "                    return token.text\n",
    "                    \n",
    "        lm_sent = [lemmatize(token) for token in tg_sent]\n",
    "        \n",
    "        if as_list:\n",
    "            return lm_sent\n",
    "        else:\n",
    "            return ' '.join(lm_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1770ac34-a7e0-42d2-9556-12bc96864da8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i die soon anded do n't i want burgers ?\n",
      "i die soon not anded do i want burgers ?\n",
      "burgers want i do anded not soon die i ?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'burgers want i do anded not soon die i ?'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagger = MorphTagger('spacy')\n",
    "rules = LinearRules(tagger)\n",
    "s = 'i died soon and didn\\'t i want burgers?'\n",
    "s = rules.shift_past(s)\n",
    "print(s)\n",
    "s = rules.shift_negation(s)\n",
    "print(s)\n",
    "s = rules.question_reverse(s)\n",
    "print(s)\n",
    "s\n",
    "# rules.question_reverse('a((?')\n",
    "# sent_tokenize('a? b.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "50f26813-c806-454b-9de9-6e50f81c5cb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:03<00:00,  2.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    Rajanakatti is a village in Belagavi district ...\n",
      "1    The great selling not point of Hinks lamps was...\n",
      "2    Reghan Tumilty (born 26 February 1997) is a Sc...\n",
      "3    Belen Belediyespor is a football club located ...\n",
      "4    German submarine U-185 was a Type IXC/40 U-boa...\n",
      "5    This station does not have connections to feed...\n",
      "6    The 2003 Dwars door Vlaanderen was the 58th ed...\n",
      "7    Scapanops is distinguished not from other diss...\n",
      "8    Victor Frederick \"Vic\" Snyder (born September ...\n",
      "9    According to Hitbox not developer Woodley Nye ...\n",
      "Name: text, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm \n",
    "tqdm.pandas()\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import numpy as np\n",
    "import numpy.ma as ma\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import re\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "\n",
    "def process_text(text, f):\n",
    "    \n",
    "    res = ''\n",
    "    # tokenize into sents\n",
    "    sents = sent_tokenize(text)\n",
    "    # split headers from body text\n",
    "    sents = sum([sent.split('\\n') for sent in sents], [])\n",
    "    for sent in sents:\n",
    "        tr_sent = f(sent)\n",
    "        if tr_sent:\n",
    "            res += tr_sent + '\\n'\n",
    "            \n",
    "    # only return contentful results\n",
    "    if res != []:\n",
    "        return res\n",
    "    else: \n",
    "        return None\n",
    "    \n",
    "def process_file(fn, func, output_path):\n",
    "    \n",
    "    data = pd.read_parquet(fn)\n",
    "    processed_text = data['text'][:10].progress_apply(lambda x: process_text(x, func) if process_text(x, func) else x)\n",
    "    print(processed_text)\n",
    "    pd.DataFrame({\n",
    "        'processed_text': processed_text,\n",
    "        'text': data['text']\n",
    "    }).to_parquet(f'{output_path}/{func.__name__}_{fn}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2597f4c7-3dac-49f4-8fc9-ce1cdfdbb81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_parque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "2c4aa17f-f9d2-4348-a0c3-564c2d30292f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['train-00007-of-00042-ad702ac8373a9f6a.parquet']"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(filter(lambda x: 'parquet' in x, os.listdir('./')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "e549d218-0690-4770-bc69-ef2c41980fa5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'shift_past'"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rules.shift_past.__name__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3e433e-0b5f-4039-bea5-77b391691a61",
   "metadata": {},
   "source": [
    "## testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6509c5cd-2a13-4cc1-b3ea-f1d6b5c4b668",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Missing optional dependency 'pyarrow'. pyarrow is required for parquet support. Use pip or conda to install pyarrow.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-3e4c9a77e9d4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_parquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train-00007-of-00042-ad702ac8373a9f6a.parquet'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'pyarrow'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parquet.py\u001b[0m in \u001b[0;36mread_parquet\u001b[0;34m(path, engine, columns, use_nullable_dtypes, **kwargs)\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0mDataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m     \"\"\"\n\u001b[0;32m--> 458\u001b[0;31m     \u001b[0mimpl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    459\u001b[0m     return impl.read(\n\u001b[1;32m    460\u001b[0m         \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_nullable_dtypes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_nullable_dtypes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parquet.py\u001b[0m in \u001b[0;36mget_engine\u001b[0;34m(engine)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"pyarrow\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mPyArrowImpl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"fastparquet\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mFastParquetImpl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parquet.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mPyArrowImpl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseImpl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m         import_optional_dependency(\n\u001b[0m\u001b[1;32m    127\u001b[0m             \u001b[0;34m\"pyarrow\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextra\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pyarrow is required for parquet support.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m         )\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/compat/_optional.py\u001b[0m in \u001b[0;36mimport_optional_dependency\u001b[0;34m(name, extra, raise_on_missing, on_version)\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mraise_on_missing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: Missing optional dependency 'pyarrow'. pyarrow is required for parquet support. Use pip or conda to install pyarrow."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_parquet('train-00007-of-00042-ad702ac8373a9f6a.parquet', engine='pyarrow')\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "cd3e6e5f-f27a-4701-a3a5-f8bfc56c9913",
   "metadata": {},
   "outputs": [],
   "source": [
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "286a37d9-8180-4150-8631-7ee49d5ec320",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ernesto Korrodi (Zürich, 31 January 1870 – Leiria, 3 February 1944), was a Swiss-born architect who moved to Portugal aged 19, spending the remainder of his life there.\n",
      "\n",
      "He later adopted Portuguese citizenship, and married a Portuguese woman. He died in 1944.\n",
      "\n",
      "Main works\n",
      "He has more than 400 works in all Portugal of which the most important are:\n",
      " Castle of D. Chica\n",
      " Hotel Guadiana in the town of Vila Real de Santo António, the oldest Hotel in the Algarve.\n",
      " Restoration of Leiria Castle\n",
      " Church of Santa Catarina da Serra, Leiria (1902)\n",
      "\n",
      "External links\n",
      " \n",
      "\n",
      "20th-century Portuguese architects\n",
      "1870 births\n",
      "1944 deaths\n"
     ]
    }
   ],
   "source": [
    "print(data['text'].sample(1).values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e2f61e8b-d43b-41ee-bb28-618f8e135239",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "992dc005-e223-4b19-8c22-33213a590c31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The pale martin or pale sand martin (Riparia diluta) is a small passerine bird in the swallow family.',\n",
       " 'It is found in open habitats such as farmland, grassland and savannah, usually near water.',\n",
       " 'It is found from Central Asia to southeastern China.',\n",
       " 'The species was formerly considered a subspecies of the sand martin.',\n",
       " 'References\\n\\nRasmussen, P.C., and J.C. Anderton.',\n",
       " '2005.',\n",
       " 'Birds of South Asia.',\n",
       " 'The Ripley guide.',\n",
       " 'Volume 2: attributes and status.',\n",
       " 'Smithsonian Institution and Lynx Edicions, Washington D.C. and Barcelona.',\n",
       " 'pale martin\\nBirds of Afghanistan\\nBirds of China\\nBirds of Central Asia\\nBirds of Mongolia\\nBirds of Pakistan\\npale martin']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['sents'].sample(1).values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "effe3ffe-316a-4f06-9d14-fe7ff2c62169",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(text, f):\n",
    "    \n",
    "    res = []\n",
    "    sents = sent_tokenize(text)\n",
    "    sents = sum([sent.split('\\n') for sent in sents], [])\n",
    "    for sent in sents:\n",
    "        tr_sent = f(sent)\n",
    "        if tr_sent:\n",
    "            res.append(tr_sent)\n",
    "            \n",
    "    if res != []:\n",
    "        return res\n",
    "    else: \n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4594de26-52da-4083-9e19-a6d2caf216bd",
   "metadata": {},
   "source": [
    "## reverse questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "d3f3b0e1-81c9-4463-bbce-310bc214b800",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:08<00:00, 1112.50it/s]\n"
     ]
    }
   ],
   "source": [
    "q_reversed = data['text'][:10_000].progress_apply(lambda x: process_text(x, rules.question_reverse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "a22f0223-0a52-43e4-9b1a-157c63e36e01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(190,)"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_reversed[~q_reversed.isnull()].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "21dee9d2-1da9-429d-90f4-e4f3725bd8a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['stores toy at buy can you ink invisible same the using spies Russian the Were , Writers Ghost ?']"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_reversed[~q_reversed.isnull()].sample(1).values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "7b107847-4a5c-47d1-8ee6-c11b5465a4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({\n",
    "    'q_reversed': q_reversed[~q_reversed.isnull()],\n",
    "    'text': data['text'][:10_000][~q_reversed.isnull()]\n",
    "}).to_csv('q_reversed.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a71c48f-4533-419d-b8b6-2e4989deb323",
   "metadata": {},
   "source": [
    "## shift negation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "0d98748e-1c7b-4e85-942e-5cf362613760",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [04:32<00:00,  3.67it/s]\n"
     ]
    }
   ],
   "source": [
    "neg_shifted = data['text'][:1000].progress_apply(lambda x: process_text(x, rules.shift_negation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "bab2d181-b8ae-4812-9c71-c3e1b5332ccf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(238,)"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_shifted[~neg_shifted.isnull()].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "56acfa6e-8144-409b-a3be-5aeba091d5e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this eucalypt be classify not as \" threaten \" by the western Australian Government Department of Parks and Wildlife .']"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_shifted[~neg_shifted.isnull()].sample(1).values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "8b7af811-7ec0-4111-bbf2-6f77cca55197",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({\n",
    "    'neg_shifted': neg_shifted[~neg_shifted.isnull()],\n",
    "    'text': data['text'][:1000][~neg_shifted.isnull()]\n",
    "}).to_csv('neg_shifted.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25464b93-f722-41e1-8d46-01dd78696fef",
   "metadata": {},
   "source": [
    "## shift past"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "ad78f191-8162-4bb2-8411-e0ea7090a6e2",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [02:55<00:00,  5.70it/s]\n"
     ]
    }
   ],
   "source": [
    "pst_shifted = data['text'][:1000].progress_apply(lambda x: process_text(x, rules.shift_past))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "bb721b9e-692f-428a-98da-c518e66456a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Atomix was received positively ; reviewersed note the game 's addictiveness and enjoyable gameplay , though criticize its repetitiveness .\",\n",
       " 'Development \\n\\n Amiga Format reviewed a pre - release version in its May , 1990 issue .',\n",
       " 'It was almost a completeed version of the game although it lack sound .',\n",
       " 'Reception \\n\\n Atomix receive warmed reactions from reviewers .',\n",
       " 'They state it was highlyed enjoyable and addictive despite its high difficulty level .',\n",
       " 'Reviewers also point out theed possible educational application of the game .',\n",
       " 'However , certain reviewers criticizeed the game for its repetitiveness and state that it lack replayability .',\n",
       " \"Some reviewers also write abouted the game 's unoriginality , noting similarities to earlier games , Xor and Leonardo .\",\n",
       " 'Graphics were generally considered adequateed , though not spectacular ; Zzap!64 call them \" a bit dull and repetitive \" and \" simplistic , but slick and effective \" , while CU Amiga remark that despite their simplicity , they \" create a nice , tidy display \" .',\n",
       " 'The soundtrack was found enjoyableed , though the Commodore Format reviewer consider it annoyingly repetitive .']"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pst_shifted[~pst_shifted.isnull()].sample(1).values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "13ac0e8c-f297-460b-8ab5-1ee64d8a14a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(695,)"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pst_shifted[~pst_shifted.isnull()].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "6ed29fcc-c8dc-4dcb-92a3-4bc7a781d99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({\n",
    "    'pst_shifted': pst_shifted[~pst_shifted.isnull()],\n",
    "    'text': data['text'][:1000][~pst_shifted.isnull()]\n",
    "}).to_csv('pst_shifted.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b89ddf-e29e-432a-92e4-c507b95e63c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b175eee4-f096-4aa0-8458-16429b85d41a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'datasets'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-cf8c3c8036df>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# datasets.load_dataset('olm/olm-wikipedia-20220920')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'glue'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'datasets'"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "\n",
    "# datasets.load_dataset('olm/olm-wikipedia-20220920')\n",
    "datasets.load_dataset('glue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33dbaf15-49e2-4047-92ac-ee22f8c1fc72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c4e199-bd4b-4dad-86b1-0374b68ab94e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "50b9434c-6a08-45d7-bc88-42c8f8bbc72f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not\n"
     ]
    }
   ],
   "source": [
    "model = spacy.load(\"en_core_web_sm\")\n",
    "# [token.pos_ for token in model('i love trains')]\n",
    "token = model('i can\\'t')[-1]\n",
    "print(token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "52b289ea-15c7-4aad-9659-74f5e2db85e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['whated']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagger = MorphTagger('spacy')\n",
    "tagger.load_model()\n",
    "tagger.tag_seq('whated')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "cd452f7b-43b9-488c-80ec-5ec1b9f1bac5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'not'"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token.suffix_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "fa80b57a-6545-40cd-8d92-8cc4445ca6f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token.i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "7a330e09-bba8-4e79-9798-6116729b80b7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_',\n",
       " '__bytes__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__pyx_vtable__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__unicode__',\n",
       " 'ancestors',\n",
       " 'check_flag',\n",
       " 'children',\n",
       " 'cluster',\n",
       " 'conjuncts',\n",
       " 'dep',\n",
       " 'dep_',\n",
       " 'doc',\n",
       " 'ent_id',\n",
       " 'ent_id_',\n",
       " 'ent_iob',\n",
       " 'ent_iob_',\n",
       " 'ent_kb_id',\n",
       " 'ent_kb_id_',\n",
       " 'ent_type',\n",
       " 'ent_type_',\n",
       " 'get_extension',\n",
       " 'has_dep',\n",
       " 'has_extension',\n",
       " 'has_head',\n",
       " 'has_morph',\n",
       " 'has_vector',\n",
       " 'head',\n",
       " 'i',\n",
       " 'idx',\n",
       " 'iob_strings',\n",
       " 'is_alpha',\n",
       " 'is_ancestor',\n",
       " 'is_ascii',\n",
       " 'is_bracket',\n",
       " 'is_currency',\n",
       " 'is_digit',\n",
       " 'is_left_punct',\n",
       " 'is_lower',\n",
       " 'is_oov',\n",
       " 'is_punct',\n",
       " 'is_quote',\n",
       " 'is_right_punct',\n",
       " 'is_sent_end',\n",
       " 'is_sent_start',\n",
       " 'is_space',\n",
       " 'is_stop',\n",
       " 'is_title',\n",
       " 'is_upper',\n",
       " 'lang',\n",
       " 'lang_',\n",
       " 'left_edge',\n",
       " 'lefts',\n",
       " 'lemma',\n",
       " 'lemma_',\n",
       " 'lex',\n",
       " 'lex_id',\n",
       " 'like_email',\n",
       " 'like_num',\n",
       " 'like_url',\n",
       " 'lower',\n",
       " 'lower_',\n",
       " 'morph',\n",
       " 'n_lefts',\n",
       " 'n_rights',\n",
       " 'nbor',\n",
       " 'norm',\n",
       " 'norm_',\n",
       " 'orth',\n",
       " 'orth_',\n",
       " 'pos',\n",
       " 'pos_',\n",
       " 'prefix',\n",
       " 'prefix_',\n",
       " 'prob',\n",
       " 'rank',\n",
       " 'remove_extension',\n",
       " 'right_edge',\n",
       " 'rights',\n",
       " 'sent',\n",
       " 'sent_start',\n",
       " 'sentiment',\n",
       " 'set_extension',\n",
       " 'set_morph',\n",
       " 'shape',\n",
       " 'shape_',\n",
       " 'similarity',\n",
       " 'subtree',\n",
       " 'suffix',\n",
       " 'suffix_',\n",
       " 'tag',\n",
       " 'tag_',\n",
       " 'tensor',\n",
       " 'text',\n",
       " 'text_with_ws',\n",
       " 'vector',\n",
       " 'vector_norm',\n",
       " 'vocab',\n",
       " 'whitespace_']"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "dbf549f5-1b24-4ff1-908a-929f3e295f6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train-00007-of-00042-ad702ac8373a9f6a.parquet\n",
      ".DS_Store\n",
      "q_reversed.csv\n",
      "pst_shifted.csv\n",
      "neg_shifted.csv\n",
      "processing.ipynb\n",
      ".ipynb_checkpoints\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "for fn in os.listdir('./'):\n",
    "    print(fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94af2ade-953c-4323-9861-6098f25ca26b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
